{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning using Pytorch\n",
    "(using model is pretrained)<br>\n",
    "\n",
    "1. finetuning : update all model's parameter for new task = retrain whole model\n",
    "2. feature extraction : only update the final layer weights\n",
    "\n",
    "모델마다 구조가 다르기 때문에 일반화된 finetuning 함수를 짜기는 어려움<br>\n",
    "하지만 일반화된 규칙은 있기 때문에 그때그때 custom하게 수정하면 됨<br>\n",
    "(참고 : https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version :  3.8.13 (default, Oct 21 2022, 23:50:54) \n",
      "[GCC 11.2.0]\n",
      "torch version :  1.13.0+cu117\n",
      "torchvision version :  0.14.0+cu117\n",
      "device number :  4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as Flatten\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from source import functions\n",
    "from source import cifar_dataloader\n",
    "\n",
    "\n",
    "print('python version : ', sys.version)\n",
    "print('torch version : ', torch.__version__)\n",
    "print('torchvision version : ', torchvision.__version__)\n",
    "print('device number : ', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'source.functions' from '/home/choiyj/pytorch_study/source/functions.py'>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from source import functions\n",
    "import importlib\n",
    "importlib.reload(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['resnet', 'alexnet', 'vgg', 'squeezenet', 'densenet', 'inception']\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = 10\n",
    "batch_size = 8\n",
    "num_epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tuning의 핵심\n",
    "# source.functions에 포함되어있음\n",
    "\n",
    "# def set_parameter_requires_grad(model, feature_extracting):\n",
    "#     if feature_extracting:\n",
    "#         for param in model.parameters():\n",
    "#             param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model for this run\n",
    "model_name = 'resnet'\n",
    "models, input_size = functions.get_model(model_name, pretrained=True, transfer=True, feature_extract=True, num_classes=num_classes)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(models.fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "path = './../data/'\n",
    "dataloaders_dict = cifar_dataloader.get_cifar10_dataloader(path, input_size, 16, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer\n",
    "optimizer에는 model.parameters()를 넣어주되 학습할 parameter와 학습하지 않을 parameter를 구분하여 넣어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "models = models.to(device)\n",
    "\n",
    "# optimizer = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "params_to_update = functions.get_model_parameters(models, feature_extract=True)\n",
    "optimizer = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer에 넣는 parameter는 다음과 같이 list로 넣어주면 된다.\n",
    "# params_to_update[0], params_to_update[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 1.4689 Acc: 0.4905\n",
      "val Loss: 0.8253 Acc: 0.7194\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 1.3846 Acc: 0.5244\n",
      "val Loss: 0.8455 Acc: 0.7178\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 1.3795 Acc: 0.5297\n",
      "val Loss: 0.7397 Acc: 0.7463\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 1.3740 Acc: 0.5313\n",
      "val Loss: 0.7972 Acc: 0.7329\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 1.3715 Acc: 0.5333\n",
      "val Loss: 0.7930 Acc: 0.7293\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 1.3755 Acc: 0.5322\n",
      "val Loss: 0.7769 Acc: 0.7418\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 1.3782 Acc: 0.5279\n",
      "val Loss: 0.7846 Acc: 0.7354\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 1.3660 Acc: 0.5348\n",
      "val Loss: 0.7843 Acc: 0.7343\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 1.3719 Acc: 0.5328\n",
      "val Loss: 0.7700 Acc: 0.7354\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 1.3741 Acc: 0.5296\n",
      "val Loss: 0.7501 Acc: 0.7409\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 1.3695 Acc: 0.5337\n",
      "val Loss: 0.7707 Acc: 0.7380\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 1.3680 Acc: 0.5329\n",
      "val Loss: 0.7890 Acc: 0.7284\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 1.3694 Acc: 0.5333\n",
      "val Loss: 0.7611 Acc: 0.7432\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 1.3743 Acc: 0.5311\n",
      "val Loss: 0.7557 Acc: 0.7428\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 1.3735 Acc: 0.5335\n",
      "val Loss: 0.8043 Acc: 0.7274\n",
      "\n",
      "Training complete in 13m 42s\n",
      "Best val Acc: 0.746300\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "# inception은 loss가 두가지이기 때문에 다르게 학습되므로 별도의 분기점 존재\n",
    "model_ft, hist = functions.train_model(\n",
    "    model = models, \n",
    "    dataloaders = dataloaders_dict, \n",
    "    criterion = criterion, \n",
    "    optimizer = optimizer, \n",
    "    device=device, \n",
    "    num_epochs=num_epochs, \n",
    "    is_inception=(model_name==\"inception\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scatch 모델과의 비교\n",
    "- input_size만 신경쓰면 그대로 사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scratch_model,_ = functions.get_model(model_name, pretrained=False, transfer=True, feature_extract=False, num_classes=num_classes)\n",
    "scratch_model = scratch_model.to(device)\n",
    "scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)\n",
    "scratch_criterion = nn.CrossEntropyLoss()\n",
    "_,scratch_hist = functions.train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, device, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('choi': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f1ea343d18a0e98dbd50f4a3341f5f39bd7be7d912eea72d8401c1237c2e901"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
