{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAwtlswlQFTU"
      },
      "source": [
        "# Bert_En_movie_review_finetuning\n",
        "참고 : https://gmihaila.github.io/tutorial_notebooks/bert_inner_workings/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jv0xW1GlwTej"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8jkCqui-wXNN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
        "from transformers import BertModel, BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DBxQlTXz3pSO"
      },
      "outputs": [],
      "source": [
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Lq3kMe9w0A4g"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "gtf8tzmC0GUo",
        "outputId": "13c453c1-3d3a-4910-ce17-1061ac360c3f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b4ac29ed-8ddd-43c7-924e-2b8bf964e8aa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a stirring , funny and finally transporting re...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>apparently reassembled from the cutting room f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>they presume their audience wo n't sit still f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>this is a visually stunning rumination on love...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>jonathan parker 's bartleby should have been t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b4ac29ed-8ddd-43c7-924e-2b8bf964e8aa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b4ac29ed-8ddd-43c7-924e-2b8bf964e8aa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b4ac29ed-8ddd-43c7-924e-2b8bf964e8aa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   0  1\n",
              "0  a stirring , funny and finally transporting re...  1\n",
              "1  apparently reassembled from the cutting room f...  0\n",
              "2  they presume their audience wo n't sit still f...  0\n",
              "3  this is a visually stunning rumination on love...  1\n",
              "4  jonathan parker 's bartleby should have been t...  1"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(6920, 2)\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)\n",
        "display(df.head())\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KTlbb6YK0zOF"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[0].values, df[1].values, random_state=42, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "cjk_nwzZ04EY"
      },
      "outputs": [],
      "source": [
        "def encode(data, tokenizer):\n",
        "    tokenized_text = tokenizer.encode_plus(data,\n",
        "                                        max_length=50,\n",
        "                                        add_special_tokens = True,\n",
        "                                        pad_to_max_length=True,\n",
        "                                        return_attention_mask=True)\n",
        "    return tokenized_text['input_ids'], tokenized_text['attention_mask'], tokenized_text['token_type_ids']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "AcHHFyF65BSV"
      },
      "outputs": [],
      "source": [
        "X_train_encoded = pd.Series(X_train).map(lambda x:encode(x, tokenizer))\n",
        "temp = np.concatenate(X_train_encoded.values).reshape(-1,3,50)\n",
        "train_input_ids = temp[:,0,:]\n",
        "train_attention_masks = temp[:,1,:]\n",
        "train_token_type_ids = temp[:,2,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "KEjl1Nqh5i_E"
      },
      "outputs": [],
      "source": [
        "X_test_encoded = pd.Series(X_test).map(lambda x:encode(x, tokenizer))\n",
        "temp = np.concatenate(X_test_encoded.values).reshape(-1,3,50)\n",
        "test_input_ids = temp[:,0,:]\n",
        "test_attention_masks = temp[:,1,:]\n",
        "test_token_type_ids = temp[:,2,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "NwJB1jkv_5pr"
      },
      "outputs": [],
      "source": [
        "class MapDataset(Dataset):\n",
        "    def __init__(self, input_ids, attention_masks, token_type_ids, label):\n",
        "        self.input_ids = torch.Tensor(input_ids).long()\n",
        "        self.attention_masks = torch.Tensor(attention_masks).long()\n",
        "        self.token_type_ids = torch.Tensor(token_type_ids).long()\n",
        "        self.label = torch.Tensor(label).long()\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "    def __getitem__(self,idx):\n",
        "        return (self.input_ids[idx],self.token_type_ids[idx],self.attention_masks[idx]), self.label[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ktzZvazXAo6K"
      },
      "outputs": [],
      "source": [
        "trainDS = MapDataset(train_input_ids, train_attention_masks, train_token_type_ids, y_train)\n",
        "testDS = MapDataset(test_input_ids, test_attention_masks, test_token_type_ids, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tog8jwpoBEy_",
        "outputId": "da5655c9-de5e-4659-a46a-d3f8e52509bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "#                     # You can increase this for multi-class tasks.   \n",
        "    # output_attentions = False, # Whether the model returns attentions weights.\n",
        "    # output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoAAyc6QLXoG",
        "outputId": "42ed4b8b-c2bc-4444-a090-750e9a2622d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 2])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output = model(*[torch.Tensor(train_input_ids)[:4].long(),\n",
        "                torch.Tensor(train_attention_masks)[:4].long(),\n",
        "                torch.Tensor(train_token_type_ids)[:4].long()])\n",
        "output['logits'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "C0YdKz7RNle0"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "lr = 2e-5\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "trainloader = torch.utils.data.DataLoader(trainDS, batch_size=batch_size, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testDS, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoOOg_2KOQWj",
        "outputId": "dfc7177d-2ae5-46d9-ca98-a8e6aa496b04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 of 10\n",
            "Train Loss: 0.6786 | Train Acc: 55.55%\n",
            "Val Loss: 0.6360 | Val Acc: 64.81%\n",
            "---------------------------------\n",
            "Epoch 2 of 10\n",
            "Train Loss: 0.6318 | Train Acc: 63.87%\n",
            "Val Loss: 0.5610 | Val Acc: 75.14%\n",
            "---------------------------------\n",
            "Epoch 3 of 10\n",
            "Train Loss: 0.5532 | Train Acc: 72.33%\n",
            "Val Loss: 0.5341 | Val Acc: 70.01%\n",
            "---------------------------------\n",
            "Epoch 4 of 10\n",
            "Train Loss: 0.4772 | Train Acc: 77.78%\n",
            "Val Loss: 0.5598 | Val Acc: 69.22%\n",
            "---------------------------------\n",
            "Epoch 5 of 10\n",
            "Train Loss: 0.4119 | Train Acc: 82.21%\n",
            "Val Loss: 0.4510 | Val Acc: 80.20%\n",
            "---------------------------------\n",
            "Epoch 6 of 10\n",
            "Train Loss: 0.3807 | Train Acc: 83.35%\n",
            "Val Loss: 0.4948 | Val Acc: 77.96%\n",
            "---------------------------------\n",
            "Epoch 7 of 10\n",
            "Train Loss: 0.3694 | Train Acc: 84.28%\n",
            "Val Loss: 0.4334 | Val Acc: 81.43%\n",
            "---------------------------------\n",
            "Epoch 8 of 10\n",
            "Train Loss: 0.3408 | Train Acc: 85.66%\n",
            "Val Loss: 0.4618 | Val Acc: 80.13%\n",
            "---------------------------------\n",
            "Epoch 9 of 10\n",
            "Train Loss: 0.3147 | Train Acc: 86.52%\n",
            "Val Loss: 0.4509 | Val Acc: 81.29%\n",
            "---------------------------------\n",
            "Epoch 10 of 10\n",
            "Train Loss: 0.3024 | Train Acc: 87.55%\n",
            "Val Loss: 0.5533 | Val Acc: 74.93%\n",
            "---------------------------------\n"
          ]
        }
      ],
      "source": [
        "model = model.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_correct = 0\n",
        "    train_loss = 0\n",
        "    model.train()\n",
        "    for (input_ids, attention_mask, token_type_ids), targets in trainloader:\n",
        "        input_ids, attention_mask, token_type_ids, targets = input_ids.to(device), attention_mask.to(device), token_type_ids.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(*[input_ids,attention_mask,token_type_ids])['logits']   # <-- 이 부분 때문에 skorch 구현은 하지 않음\n",
        "        loss = criterion(pred, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()*input_ids.size(0)\n",
        "        train_correct += (pred.argmax(1) == targets).sum().item()\n",
        "    train_loss /= len(trainloader.dataset)\n",
        "    train_acc = train_correct / len(trainloader.dataset)\n",
        "\n",
        "    val_correct = 0 \n",
        "    val_loss = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for (input_ids, attention_mask, token_type_ids), targets in testloader:\n",
        "            input_ids, attention_mask, token_type_ids, targets = input_ids.to(device), attention_mask.to(device), token_type_ids.to(device), targets.to(device)\n",
        "            pred = model(*[input_ids,attention_mask,token_type_ids])['logits']\n",
        "            loss = criterion(pred, targets)\n",
        "            val_loss += loss.item()*input_ids.size(0)\n",
        "            val_correct += (pred.argmax(1) == targets).sum().item()\n",
        "    val_loss /= len(testloader.dataset)\n",
        "    val_acc = val_correct / len(testloader.dataset)\n",
        "\n",
        "    print(f'Epoch {epoch+1} of {epochs}')\n",
        "    print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%')\n",
        "    print('---------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDg7RvCbSuGh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
