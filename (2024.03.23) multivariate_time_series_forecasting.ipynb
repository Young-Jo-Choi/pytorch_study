{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- 시계열 데이터에 대한 예측\n",
        "    - HEPC(house hold electric power consumption) dataset : 다변량 시계열 데이터\n",
        "- tensorflow로 모델 개발\n",
        "- torch로 동일한 모델 개발"
      ],
      "metadata": {
        "id": "2aY0-oXABdCT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HEPC dataset\n",
        "모든 컬럼에 대한 forecasting을 수행"
      ],
      "metadata": {
        "id": "1TnVMPFCKAxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import urllib\n",
        "import zipfile\n",
        "\n",
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/household_power.zip'\n",
        "urllib.request.urlretrieve(url, 'household_power.zip')\n",
        "with zipfile.ZipFile('household_power.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "hepc = pd.read_csv('household_power_consumption.csv')\n",
        "print(hepc.shape)\n",
        "hepc.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "bbFNf7wOHZQW",
        "outputId": "31c819b7-0377-44e2-ceb6-1ab7e8a89b16"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(86400, 8)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              datetime  Global_active_power  Global_reactive_power  Voltage  \\\n",
              "0  2006-12-16 17:24:00                4.216                  0.418   234.84   \n",
              "1  2006-12-16 17:25:00                5.360                  0.436   233.63   \n",
              "2  2006-12-16 17:26:00                5.374                  0.498   233.29   \n",
              "3  2006-12-16 17:27:00                5.388                  0.502   233.74   \n",
              "4  2006-12-16 17:28:00                3.666                  0.528   235.68   \n",
              "\n",
              "   Global_intensity  Sub_metering_1  Sub_metering_2  Sub_metering_3  \n",
              "0              18.4             0.0             1.0            17.0  \n",
              "1              23.0             0.0             1.0            16.0  \n",
              "2              23.0             0.0             2.0            17.0  \n",
              "3              23.0             0.0             1.0            17.0  \n",
              "4              15.8             0.0             1.0            17.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b1ed940d-47cb-401d-8221-cc003acf9793\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>datetime</th>\n",
              "      <th>Global_active_power</th>\n",
              "      <th>Global_reactive_power</th>\n",
              "      <th>Voltage</th>\n",
              "      <th>Global_intensity</th>\n",
              "      <th>Sub_metering_1</th>\n",
              "      <th>Sub_metering_2</th>\n",
              "      <th>Sub_metering_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2006-12-16 17:24:00</td>\n",
              "      <td>4.216</td>\n",
              "      <td>0.418</td>\n",
              "      <td>234.84</td>\n",
              "      <td>18.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>17.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2006-12-16 17:25:00</td>\n",
              "      <td>5.360</td>\n",
              "      <td>0.436</td>\n",
              "      <td>233.63</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2006-12-16 17:26:00</td>\n",
              "      <td>5.374</td>\n",
              "      <td>0.498</td>\n",
              "      <td>233.29</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>17.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2006-12-16 17:27:00</td>\n",
              "      <td>5.388</td>\n",
              "      <td>0.502</td>\n",
              "      <td>233.74</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>17.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2006-12-16 17:28:00</td>\n",
              "      <td>3.666</td>\n",
              "      <td>0.528</td>\n",
              "      <td>235.68</td>\n",
              "      <td>15.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>17.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b1ed940d-47cb-401d-8221-cc003acf9793')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b1ed940d-47cb-401d-8221-cc003acf9793 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b1ed940d-47cb-401d-8221-cc003acf9793');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-97c009c7-bd79-4cba-b2b6-b52493565eb7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-97c009c7-bd79-4cba-b2b6-b52493565eb7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-97c009c7-bd79-4cba-b2b6-b52493565eb7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "hepc",
              "summary": "{\n  \"name\": \"hepc\",\n  \"rows\": 86400,\n  \"fields\": [\n    {\n      \"column\": \"datetime\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 86400,\n        \"samples\": [\n          \"2007-01-15 20:48:00\",\n          \"2007-02-09 02:41:00\",\n          \"2006-12-20 23:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Global_active_power\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.335541718985206,\n        \"min\": 0.194,\n        \"max\": 9.272,\n        \"num_unique_values\": 3347,\n        \"samples\": [\n          5.406,\n          5.626,\n          3.904\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Global_reactive_power\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1176214630431444,\n        \"min\": 0.0,\n        \"max\": 0.874,\n        \"num_unique_values\": 377,\n        \"samples\": [\n          0.656,\n          0.466,\n          0.632\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Voltage\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.498535931183726,\n        \"min\": 224.68,\n        \"max\": 251.7,\n        \"num_unique_values\": 2174,\n        \"samples\": [\n          238.73,\n          248.19,\n          237.56\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Global_intensity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.629462938587408,\n        \"min\": 0.8,\n        \"max\": 40.4,\n        \"num_unique_values\": 189,\n        \"samples\": [\n          37.4,\n          36.2,\n          13.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sub_metering_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.682567482882876,\n        \"min\": 0.0,\n        \"max\": 77.0,\n        \"num_unique_values\": 60,\n        \"samples\": [\n          0.0,\n          36.0,\n          14.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sub_metering_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.5676794757081645,\n        \"min\": 0.0,\n        \"max\": 78.0,\n        \"num_unique_values\": 77,\n        \"samples\": [\n          37.0,\n          40.0,\n          9.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sub_metering_3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.67190930406459,\n        \"min\": 0.0,\n        \"max\": 20.0,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          17.0,\n          12.0,\n          6.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize\n",
        "data_origin = hepc.values[:,1:]\n",
        "data = data_origin.astype('float32')\n",
        "data = (data - data.min(axis=0)) / (data.max(axis=0) - data.min(axis=0))\n",
        "n_features = data.shape[1]\n"
      ],
      "metadata": {
        "id": "J9gWDpphJhUH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train, valid split\n",
        "train_size = int(len(data) * 0.5)\n",
        "train_data, valid_data = data[:train_size, :], data[train_size:, :]"
      ],
      "metadata": {
        "id": "7EQFPl1N2fT7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tensorflow"
      ],
      "metadata": {
        "id": "w-_BaK1dJ-U1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def windowed_dataset(series, batch_size, n_past=24, n_future=24, shift=1):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(size= (n_past + n_future),shift = shift, drop_remainder = True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
        "    ds = ds.map(\n",
        "        lambda w: (w[:n_past], w[n_past:])\n",
        "    )\n",
        "    return ds.batch(batch_size).prefetch(1)"
      ],
      "metadata": {
        "id": "ixpIsPXBJhQw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "\n",
        "# 0~23의 데이터를 이용해 뒤의 24~47의 데이터를 예측함\n",
        "n_past = 24\n",
        "n_future = 24\n",
        "batch_size = 32\n",
        "# window_size = n_past + n_future\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(51)\n",
        "np.random.seed(51)\n",
        "\n",
        "train_set = windowed_dataset(train_data, batch_size, n_past, n_future)\n",
        "valid_set = windowed_dataset(valid_data, batch_size, n_past, n_future)"
      ],
      "metadata": {
        "id": "4YTwiN9RJhOT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,(x_tf,y_tf) in enumerate(train_set.take(1)):\n",
        "    if i==1:\n",
        "        break"
      ],
      "metadata": {
        "id": "nxbMTUmoOvgQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_tf.shape, y_tf.shape"
      ],
      "metadata": {
        "id": "ePrZOY1-PAOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dacb42e4-509a-4c1b-a7a4-a3f9c97ad996"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([32, 24, 7]), TensorShape([32, 24, 7]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# first batch\n",
        "x_tf[0,:10,0], y_tf[0,:10,0]"
      ],
      "metadata": {
        "id": "CVg4FMf5SsNI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69a53fc1-4ec8-4217-890d-f46e0e3772b3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
              " array([0.44304916, 0.56906813, 0.5706103 , 0.5721525 , 0.38246307,\n",
              "        0.36638024, 0.3864287 , 0.38620842, 0.3826834 , 0.38202247],\n",
              "       dtype=float32)>,\n",
              " <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
              " array([0.47146946, 0.3364177 , 0.33509585, 0.33421457, 0.33751926,\n",
              "        0.32870677, 0.27825513, 0.39259747, 0.45692882, 0.4756554 ],\n",
              "       dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[:10,0], train_data[n_past:n_past+10,0]"
      ],
      "metadata": {
        "id": "RXbFXMImTvbZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68285947-cc89-4b74-e386-2ef368889803"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.44304916, 0.56906813, 0.5706103 , 0.5721525 , 0.38246307,\n",
              "        0.36638024, 0.3864287 , 0.38620842, 0.3826834 , 0.38202247],\n",
              "       dtype=float32),\n",
              " array([0.47146946, 0.3364177 , 0.33509585, 0.33421457, 0.33751926,\n",
              "        0.32870677, 0.27825513, 0.39259747, 0.45692882, 0.4756554 ],\n",
              "       dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def equal_two_tensor(a,b):\n",
        "    aa,_ = tf.unique(tf.reshape(a==b,-1))\n",
        "    return aa.numpy()\n",
        "equal_two_tensor(y_tf[0,:,:],train_data[n_past:n_past+n_future, :])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVcIPgMd9Y6W",
        "outputId": "4261e70b-4c2f-46f7-e149-cf434fad98e9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 참고\n",
        "print(x_tf.shape)\n",
        "x_conv_tf = keras.layers.Conv1D(filters=32, kernel_size=3, padding='causal', activation='relu', input_shape=[n_past, n_features])(x_tf)\n",
        "print(x_conv_tf.shape)\n",
        "# Bidrectional은 양방향으로 학습할 수 있게 함\n",
        "x_lstm1_tf = Bidirectional(keras.layers.LSTM(64, return_sequences=True))(x_conv_tf)\n",
        "print(x_lstm1_tf.shape)\n",
        "x_lstm2_tf = Bidirectional(keras.layers.LSTM(64, return_sequences=True))(x_lstm1_tf)\n",
        "print(x_lstm2_tf.shape)\n",
        "x_dense1_tf = keras.layers.Dense(32, activation='relu')(x_lstm2_tf)\n",
        "print(x_dense1_tf.shape)\n",
        "\n",
        "# (32,24,128)은 batch size : 32, sequence_length (n_past) : 24, feature의 개수 64*2 를 의미함"
      ],
      "metadata": {
        "id": "hYHn33zAo4MZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab56da41-c8e2-4ee3-a2c9-a33d33238165"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 24, 7)\n",
            "(32, 24, 32)\n",
            "(32, 24, 128)\n",
            "(32, 24, 128)\n",
            "(32, 24, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    # causal padding은 시계열 데이터에서 미래의 데이터를 미리 보는 일을 방지하기 위해 과거 데이터쪽으로만 padding을 추가\n",
        "    keras.layers.Conv1D(filters=32, kernel_size=3, padding='causal', activation='relu', input_shape=[n_past, n_features]),\n",
        "    Bidirectional(keras.layers.LSTM(64, return_sequences=True)),\n",
        "    Bidirectional(keras.layers.LSTM(64, return_sequences=True)),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.Dense(16, activation='relu'),\n",
        "    keras.layers.Dense(n_features)\n",
        "])\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.0005)\n",
        "model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])\n",
        "\n",
        "# callback은 생략\n",
        "model.fit(train_set, epochs=20, validation_data=valid_set)"
      ],
      "metadata": {
        "id": "RH7M8i7SJhLJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71308484-2f80-45ca-ee32-6c8ad1b823fd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1349/1349 [==============================] - 28s 16ms/step - loss: 0.1000 - mae: 0.1000 - val_loss: 0.0706 - val_mae: 0.0706\n",
            "Epoch 2/20\n",
            "1349/1349 [==============================] - 20s 15ms/step - loss: 0.0681 - mae: 0.0681 - val_loss: 0.0627 - val_mae: 0.0627\n",
            "Epoch 3/20\n",
            "1349/1349 [==============================] - 20s 15ms/step - loss: 0.0619 - mae: 0.0619 - val_loss: 0.0597 - val_mae: 0.0597\n",
            "Epoch 4/20\n",
            "1349/1349 [==============================] - 21s 15ms/step - loss: 0.0603 - mae: 0.0603 - val_loss: 0.0599 - val_mae: 0.0599\n",
            "Epoch 5/20\n",
            "1349/1349 [==============================] - 21s 15ms/step - loss: 0.0585 - mae: 0.0585 - val_loss: 0.0580 - val_mae: 0.0580\n",
            "Epoch 6/20\n",
            "1349/1349 [==============================] - 20s 15ms/step - loss: 0.0575 - mae: 0.0575 - val_loss: 0.0585 - val_mae: 0.0585\n",
            "Epoch 7/20\n",
            "1349/1349 [==============================] - 20s 15ms/step - loss: 0.0568 - mae: 0.0568 - val_loss: 0.0576 - val_mae: 0.0576\n",
            "Epoch 8/20\n",
            "1349/1349 [==============================] - 21s 15ms/step - loss: 0.0556 - mae: 0.0556 - val_loss: 0.0590 - val_mae: 0.0590\n",
            "Epoch 9/20\n",
            "1349/1349 [==============================] - 21s 15ms/step - loss: 0.0549 - mae: 0.0549 - val_loss: 0.0558 - val_mae: 0.0558\n",
            "Epoch 10/20\n",
            "1349/1349 [==============================] - 20s 15ms/step - loss: 0.0556 - mae: 0.0556 - val_loss: 0.0553 - val_mae: 0.0553\n",
            "Epoch 11/20\n",
            "1349/1349 [==============================] - 21s 15ms/step - loss: 0.0535 - mae: 0.0535 - val_loss: 0.0539 - val_mae: 0.0539\n",
            "Epoch 12/20\n",
            "1349/1349 [==============================] - 21s 15ms/step - loss: 0.0531 - mae: 0.0531 - val_loss: 0.0536 - val_mae: 0.0536\n",
            "Epoch 13/20\n",
            "1349/1349 [==============================] - 21s 16ms/step - loss: 0.0528 - mae: 0.0528 - val_loss: 0.0532 - val_mae: 0.0532\n",
            "Epoch 14/20\n",
            "1349/1349 [==============================] - 21s 15ms/step - loss: 0.0526 - mae: 0.0526 - val_loss: 0.0540 - val_mae: 0.0540\n",
            "Epoch 15/20\n",
            "1349/1349 [==============================] - 21s 15ms/step - loss: 0.0529 - mae: 0.0529 - val_loss: 0.0539 - val_mae: 0.0539\n",
            "Epoch 16/20\n",
            "1349/1349 [==============================] - 21s 15ms/step - loss: 0.0528 - mae: 0.0528 - val_loss: 0.0542 - val_mae: 0.0542\n",
            "Epoch 17/20\n",
            "1349/1349 [==============================] - 21s 15ms/step - loss: 0.0515 - mae: 0.0515 - val_loss: 0.0547 - val_mae: 0.0547\n",
            "Epoch 18/20\n",
            "1349/1349 [==============================] - 20s 15ms/step - loss: 0.0514 - mae: 0.0514 - val_loss: 0.0527 - val_mae: 0.0527\n",
            "Epoch 19/20\n",
            "1349/1349 [==============================] - 21s 15ms/step - loss: 0.0512 - mae: 0.0512 - val_loss: 0.0527 - val_mae: 0.0527\n",
            "Epoch 20/20\n",
            "1349/1349 [==============================] - 21s 15ms/step - loss: 0.0510 - mae: 0.0510 - val_loss: 0.0529 - val_mae: 0.0529\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e719bfb2ce0>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_mae = 0\n",
        "i = 100\n",
        "count = 0\n",
        "y_mean_list = []\n",
        "y_pred_mean_list = []\n",
        "for x, y in valid_set.take(i):\n",
        "    y_pred = model(x)\n",
        "    y_mean_list.append(y.numpy().mean())\n",
        "    y_pred_mean_list.append(y_pred.numpy().mean())\n",
        "    mae = np.mean(np.abs(y.numpy() - y_pred.numpy()))\n",
        "    total_mae += mae\n",
        "    count += 1\n",
        "total_mae /= count\n",
        "print(y_mean_list)\n",
        "print(y_pred_mean_list)\n",
        "print(total_mae)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23OEBno02FOo",
        "outputId": "d3ae22bc-3c85-47d7-bbc2-5d535c21c2ee"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.2860595, 0.25557086, 0.2915365, 0.33576047, 0.30199873, 0.24839891, 0.15883857, 0.14882076, 0.18442619, 0.22276969, 0.26605153, 0.16713813, 0.13072902, 0.122702315, 0.10736147, 0.12271933, 0.13360439, 0.11928638, 0.11397511, 0.14978066, 0.23389189, 0.11963125, 0.112713635, 0.1329354, 0.2616552, 0.2919368, 0.25043818, 0.15243855, 0.12106822, 0.17064217, 0.12551469, 0.096363306, 0.10921805, 0.13164897, 0.24689206, 0.17574838, 0.16144931, 0.2365964, 0.28801113, 0.29241395, 0.1434612, 0.22007643, 0.29401255, 0.27756903, 0.34108487, 0.3103871, 0.29502532, 0.19380753, 0.13383748, 0.1317888, 0.14714314, 0.1359688, 0.1300733, 0.15195613, 0.16430189, 0.12833779, 0.120940775, 0.12662578, 0.10506235, 0.102397546, 0.17274241, 0.22020787, 0.11293553, 0.11474011, 0.12069455, 0.11047538, 0.10510377, 0.10348781, 0.13292213, 0.2699859, 0.27930564, 0.3259995, 0.37528682, 0.4039551, 0.37856394, 0.44582748, 0.3618268, 0.31080756, 0.31423458, 0.2859264, 0.27753252, 0.29675362, 0.27574727, 0.28368, 0.43714213, 0.3996413, 0.3217523, 0.31388846, 0.3649634, 0.3090013, 0.15348923, 0.12801874, 0.16160758, 0.16897985, 0.23857108, 0.3233241, 0.31924656, 0.35453534, 0.29731014, 0.3227841]\n",
            "[0.28019673, 0.2836109, 0.26182896, 0.3089972, 0.2935965, 0.2913585, 0.16584133, 0.14516851, 0.15954699, 0.16849841, 0.21173255, 0.17475311, 0.13880801, 0.13470483, 0.120411135, 0.13081753, 0.1362525, 0.13516499, 0.13059853, 0.12523463, 0.24932769, 0.16662118, 0.1283496, 0.1325874, 0.18636608, 0.292979, 0.29245305, 0.14243901, 0.13148527, 0.1357343, 0.13610809, 0.113226414, 0.11343931, 0.13594922, 0.1500548, 0.15445635, 0.14630209, 0.17850694, 0.29378352, 0.29654518, 0.1880269, 0.14992502, 0.29742855, 0.2980786, 0.28459316, 0.30722028, 0.29594022, 0.24674451, 0.13563454, 0.13833182, 0.14138547, 0.13957866, 0.14565122, 0.14424017, 0.16162297, 0.14367458, 0.13589494, 0.13288845, 0.12244044, 0.118047796, 0.1236945, 0.27809423, 0.13100728, 0.12926094, 0.12653582, 0.12669902, 0.12213818, 0.11825187, 0.11894653, 0.20364892, 0.29135156, 0.28715062, 0.31923375, 0.3097338, 0.30388227, 0.31010604, 0.334454, 0.29855597, 0.2875876, 0.28607768, 0.2912055, 0.28999132, 0.28778854, 0.28768408, 0.31219485, 0.35541672, 0.31759095, 0.30779228, 0.30276543, 0.3369739, 0.172876, 0.14150521, 0.13921551, 0.15168364, 0.1592249, 0.30738473, 0.30115753, 0.2995914, 0.2984608, 0.2924307]\n",
            "0.05431757718324661\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pytorch"
      ],
      "metadata": {
        "id": "OTLk2Of0UawY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, series, n_past, n_future):\n",
        "        self.series = series\n",
        "        self.n_past = n_past\n",
        "        self.n_future = n_future\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.series.shape[0] - self.n_past - self.n_future\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.series[idx : idx+self.n_past, :]\n",
        "        y = self.series[idx+self.n_past : idx+self.n_past+self.n_future, :]\n",
        "        return X, y\n",
        "\n",
        "n_past = 24\n",
        "n_future = 24\n",
        "timeSeries_DS = TimeSeriesDataset(train_data, n_past, n_future)\n",
        "x_torch, y_torch = timeSeries_DS[0]"
      ],
      "metadata": {
        "id": "6SBWQxWFUce4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timeSeries_loader = DataLoader(timeSeries_DS, batch_size=32, shuffle=True)\n",
        "x_torch,y_torch = next(iter(timeSeries_loader))\n",
        "x_torch.shape, y_torch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SizrGx_iYTQ",
        "outputId": "c126d8a7-fe19-4f5e-8aef-3ccb794d76d4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 24, 7]), torch.Size([32, 24, 7]))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# causal padding을 지원하는 conv1d 층 구현\n",
        "class CausalConv1d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, **kwargs)\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        # kernel_size - 1 만큼 패딩을 오른쪽(미래 데이터 측)에 추가\n",
        "        # input : (batch, channel, sequence)\n",
        "        padding = (0,self.kernel_size - 1,0,0)\n",
        "        # pad has the form (padding_left,padding_right,padding_top,padding_bottom)\n",
        "        x = F.pad(x, pad=padding, mode='constant')  # 'constant'는 0으로 패딩한다는 의미\n",
        "        return self.conv(x)"
      ],
      "metadata": {
        "id": "NjeNa6lkgIoo"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 참고\n",
        "F.pad(x_torch, pad=(0,0,0,4), mode='constant').shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z76jRB0ZmE5R",
        "outputId": "9ea19a73-e3cf-4cfe-8126-5277fd0cb796"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 28, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 참고\n",
        "print(x_torch.shape)\n",
        "print(x_torch.permute(0,2,1).shape)\n",
        "temp = CausalConv1d(in_channels=7, out_channels=32, kernel_size=3)\n",
        "x_conv_torch = temp(x_torch.permute(0,2,1).float())\n",
        "print(x_conv_torch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtDk4tcBgvM0",
        "outputId": "0ce7eafd-38ac-4f99-cf77-628efdad8ad8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 24, 7])\n",
            "torch.Size([32, 7, 24])\n",
            "torch.Size([32, 32, 24])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 참고\n",
        "# Note: batch_first=True makes the input and output tensors of shape (batch, seq, feature)\n",
        "print(x_conv_torch.permute(0,2,1).shape)\n",
        "x_lstm1_torch, _ = nn.LSTM(input_size=32, hidden_size=64, batch_first=True)(x_conv_torch.permute(0,2,1))\n",
        "print(x_lstm1_torch.shape)\n",
        "x_lstm2_torch, _ = nn.LSTM(input_size=64, hidden_size=64, batch_first=True)(x_lstm1_torch)\n",
        "print(x_lstm2_torch.shape)\n",
        "x_dense1_torch = nn.Linear(in_features=64, out_features=32)(x_lstm2_torch)\n",
        "print(x_dense1_torch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_dhyAxpoDmu",
        "outputId": "92242424-4781-48ac-cbec-289d68e998f4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 24, 32])\n",
            "torch.Size([32, 24, 64])\n",
            "torch.Size([32, 24, 64])\n",
            "torch.Size([32, 24, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class timeSeriesModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1d = CausalConv1d(in_channels=7, out_channels=32, kernel_size=3)\n",
        "\n",
        "        # Note: batch_first=True makes the input and output tensors of shape (batch, seq, feature)\n",
        "        self.lstm1 = nn.LSTM(input_size=32, hidden_size=64, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(input_size=64, hidden_size=64, batch_first=True)\n",
        "        self.dense1 = nn.Linear(in_features=64, out_features=32)\n",
        "        self.dense2 = nn.Linear(in_features=32, out_features=16)\n",
        "        self.dense3 = nn.Linear(in_features=16, out_features=7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim()==2:\n",
        "            # channel\n",
        "            x = x.unsqueeze(1)\n",
        "        x = x.permute(0,2,1)\n",
        "        x = torch.relu(self.conv1d(x.float()))\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x, _ = self.lstm1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x = torch.relu(self.dense1(x))\n",
        "        x = torch.relu(self.dense2(x))\n",
        "        x = self.dense3(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "MexBptPNYwHI"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timeSeries_model = timeSeriesModel()\n",
        "x_final = timeSeries_model(x_torch)\n",
        "x_final.shape"
      ],
      "metadata": {
        "id": "ypxRVhGwZFBA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f081ce5b-68ea-461a-c56d-d5e9ec81c670"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 24, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_DS = TimeSeriesDataset(train_data, n_past, n_future)\n",
        "train_loader = DataLoader(train_DS, batch_size=32, shuffle=True)\n",
        "val_DS = TimeSeriesDataset(valid_data, n_past, n_future)\n",
        "val_loader = DataLoader(val_DS, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "P6MRnxXYw5Le"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timeSeries_model = timeSeriesModel()\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "timeSeries_model.to(device)\n",
        "optimizer = torch.optim.Adam(timeSeries_model.parameters(), lr=0.0005)\n",
        "criterion = nn.L1Loss().to(device)"
      ],
      "metadata": {
        "id": "H75RjSv9ZYX5"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 40\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0\n",
        "    train_mae = 0\n",
        "    timeSeries_model.train()\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs = inputs.float().to(device)\n",
        "        targets = targets.float().to(device)\n",
        "        outputs = timeSeries_model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss +=  loss.item() * inputs.size(0)\n",
        "        train_mae += torch.mean(torch.abs(targets - outputs)) * inputs.size(0)\n",
        "    train_loss /= len(train_DS)\n",
        "    train_mae /= len(train_DS)\n",
        "\n",
        "\n",
        "    val_loss = 0\n",
        "    val_mae = 0\n",
        "    timeSeries_model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs = inputs.float().to(device)\n",
        "            targets = targets.float().to(device)\n",
        "            outputs = timeSeries_model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            val_mae += torch.mean(torch.abs(targets - outputs)) * inputs.size(0)\n",
        "    val_loss /= len(val_DS)\n",
        "    val_mae /= len(val_DS)\n",
        "\n",
        "    print(f'''Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.6f}, Train MAE: {train_mae:.6f}, Val Loss: {val_loss:.6f}, Val MAE: {val_mae:.6f}''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hw5F7f4jZYVr",
        "outputId": "251c387f-1614-4f2b-8c5f-a91fa5a9dd5f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40, Train Loss: 0.088813, Train MAE: 0.088813, Val Loss: 0.065085, Val MAE: 0.065085\n",
            "Epoch 2/40, Train Loss: 0.062726, Train MAE: 0.062726, Val Loss: 0.062204, Val MAE: 0.062204\n",
            "Epoch 3/40, Train Loss: 0.061008, Train MAE: 0.061008, Val Loss: 0.061780, Val MAE: 0.061780\n",
            "Epoch 4/40, Train Loss: 0.060435, Train MAE: 0.060435, Val Loss: 0.061539, Val MAE: 0.061539\n",
            "Epoch 5/40, Train Loss: 0.060077, Train MAE: 0.060077, Val Loss: 0.060900, Val MAE: 0.060900\n",
            "Epoch 6/40, Train Loss: 0.059795, Train MAE: 0.059795, Val Loss: 0.061150, Val MAE: 0.061150\n",
            "Epoch 7/40, Train Loss: 0.059468, Train MAE: 0.059468, Val Loss: 0.060550, Val MAE: 0.060550\n",
            "Epoch 8/40, Train Loss: 0.058847, Train MAE: 0.058847, Val Loss: 0.059850, Val MAE: 0.059850\n",
            "Epoch 9/40, Train Loss: 0.058416, Train MAE: 0.058416, Val Loss: 0.059465, Val MAE: 0.059465\n",
            "Epoch 10/40, Train Loss: 0.058136, Train MAE: 0.058136, Val Loss: 0.059169, Val MAE: 0.059169\n",
            "Epoch 11/40, Train Loss: 0.057902, Train MAE: 0.057902, Val Loss: 0.059313, Val MAE: 0.059313\n",
            "Epoch 12/40, Train Loss: 0.057673, Train MAE: 0.057673, Val Loss: 0.059387, Val MAE: 0.059387\n",
            "Epoch 13/40, Train Loss: 0.057443, Train MAE: 0.057443, Val Loss: 0.059883, Val MAE: 0.059883\n",
            "Epoch 14/40, Train Loss: 0.057251, Train MAE: 0.057251, Val Loss: 0.060355, Val MAE: 0.060355\n",
            "Epoch 15/40, Train Loss: 0.056922, Train MAE: 0.056922, Val Loss: 0.059384, Val MAE: 0.059384\n",
            "Epoch 16/40, Train Loss: 0.056716, Train MAE: 0.056716, Val Loss: 0.059015, Val MAE: 0.059015\n",
            "Epoch 17/40, Train Loss: 0.056520, Train MAE: 0.056520, Val Loss: 0.058848, Val MAE: 0.058848\n",
            "Epoch 18/40, Train Loss: 0.056340, Train MAE: 0.056340, Val Loss: 0.058664, Val MAE: 0.058663\n",
            "Epoch 19/40, Train Loss: 0.056168, Train MAE: 0.056168, Val Loss: 0.059698, Val MAE: 0.059698\n",
            "Epoch 20/40, Train Loss: 0.055982, Train MAE: 0.055982, Val Loss: 0.059009, Val MAE: 0.059009\n",
            "Epoch 21/40, Train Loss: 0.055735, Train MAE: 0.055735, Val Loss: 0.059374, Val MAE: 0.059374\n",
            "Epoch 22/40, Train Loss: 0.055580, Train MAE: 0.055580, Val Loss: 0.059543, Val MAE: 0.059543\n",
            "Epoch 23/40, Train Loss: 0.055376, Train MAE: 0.055376, Val Loss: 0.059370, Val MAE: 0.059370\n",
            "Epoch 24/40, Train Loss: 0.055302, Train MAE: 0.055302, Val Loss: 0.059264, Val MAE: 0.059264\n",
            "Epoch 25/40, Train Loss: 0.055635, Train MAE: 0.055635, Val Loss: 0.059483, Val MAE: 0.059483\n",
            "Epoch 26/40, Train Loss: 0.055141, Train MAE: 0.055141, Val Loss: 0.060035, Val MAE: 0.060035\n",
            "Epoch 27/40, Train Loss: 0.054879, Train MAE: 0.054879, Val Loss: 0.059043, Val MAE: 0.059043\n",
            "Epoch 28/40, Train Loss: 0.054809, Train MAE: 0.054809, Val Loss: 0.059405, Val MAE: 0.059405\n",
            "Epoch 29/40, Train Loss: 0.054347, Train MAE: 0.054347, Val Loss: 0.060576, Val MAE: 0.060576\n",
            "Epoch 30/40, Train Loss: 0.054113, Train MAE: 0.054113, Val Loss: 0.059836, Val MAE: 0.059836\n",
            "Epoch 31/40, Train Loss: 0.054045, Train MAE: 0.054045, Val Loss: 0.060669, Val MAE: 0.060669\n",
            "Epoch 32/40, Train Loss: 0.054054, Train MAE: 0.054054, Val Loss: 0.060113, Val MAE: 0.060113\n",
            "Epoch 33/40, Train Loss: 0.053970, Train MAE: 0.053970, Val Loss: 0.059340, Val MAE: 0.059340\n",
            "Epoch 34/40, Train Loss: 0.053818, Train MAE: 0.053818, Val Loss: 0.059120, Val MAE: 0.059120\n",
            "Epoch 35/40, Train Loss: 0.053380, Train MAE: 0.053380, Val Loss: 0.059670, Val MAE: 0.059670\n",
            "Epoch 36/40, Train Loss: 0.053235, Train MAE: 0.053235, Val Loss: 0.059635, Val MAE: 0.059635\n",
            "Epoch 37/40, Train Loss: 0.053113, Train MAE: 0.053113, Val Loss: 0.060547, Val MAE: 0.060547\n",
            "Epoch 38/40, Train Loss: 0.052950, Train MAE: 0.052950, Val Loss: 0.060082, Val MAE: 0.060082\n",
            "Epoch 39/40, Train Loss: 0.052517, Train MAE: 0.052517, Val Loss: 0.060031, Val MAE: 0.060031\n",
            "Epoch 40/40, Train Loss: 0.052366, Train MAE: 0.052366, Val Loss: 0.060495, Val MAE: 0.060495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_mae = 0\n",
        "counts = 0\n",
        "target_mean_list = []\n",
        "output_mean_list = []\n",
        "for inputs, targets in val_loader:\n",
        "    inputs = inputs.float().to(device)\n",
        "    targets = targets.float().to(device)\n",
        "    outputs = timeSeries_model(inputs).squeeze()\n",
        "    target_mean_list.append(targets.detach().cpu().numpy().mean())\n",
        "    output_mean_list.append(outputs.detach().cpu().numpy().mean())\n",
        "    mae = np.mean(np.abs(targets.cpu().detach().numpy() - outputs.detach().cpu().numpy()))\n",
        "    total_mae += mae\n",
        "    counts += 1\n",
        "total_mae /= counts\n",
        "print(target_mean_list)\n",
        "print(output_mean_list)\n",
        "print(total_mae)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4VDBoKi4IBX",
        "outputId": "68ab7037-29d8-4dd5-c55b-bc9424dceec0"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.23461537, 0.19872873, 0.20178634, 0.195664, 0.19769846, 0.20283091, 0.22764537, 0.22097366, 0.19416931, 0.2230777, 0.21878515, 0.20148121, 0.1967176, 0.2378909, 0.21748643, 0.23510988, 0.22603276, 0.21181262, 0.21367775, 0.22918728, 0.22857194, 0.20009504, 0.2574101, 0.20225824, 0.21173732, 0.23255175, 0.21913068, 0.19301614, 0.18378003, 0.23269184, 0.20177065, 0.20406936, 0.22605458, 0.19131133, 0.22746618, 0.18120235, 0.23097104, 0.21101388, 0.21512127, 0.20871721, 0.20194213, 0.18282367, 0.21073878, 0.20016906, 0.22101666, 0.18818219, 0.20109771, 0.22813699, 0.19895667, 0.21883193, 0.21564497, 0.21292904, 0.22860005, 0.21225747, 0.23690021, 0.23650767, 0.19567338, 0.20268172, 0.22001033, 0.22670043, 0.21469364, 0.2009381, 0.19540326, 0.21679701, 0.2023739, 0.19257255, 0.22623278, 0.21731104, 0.19673021, 0.24018122, 0.2071915, 0.24090421, 0.20542772, 0.21612163, 0.19328694, 0.18795039, 0.2131137, 0.1880823, 0.18933189, 0.20019683, 0.22626643, 0.2206957, 0.23206902, 0.21899574, 0.20901525, 0.2326949, 0.20215043, 0.21434763, 0.20708106, 0.19524328, 0.21828122, 0.20151666, 0.1999295, 0.18316448, 0.22328627, 0.19315717, 0.20257524, 0.21662803, 0.21961442, 0.19685663, 0.19460046, 0.21637878, 0.20343772, 0.19352701, 0.19869392, 0.23691842, 0.21277834, 0.24124177, 0.24023941, 0.21618444, 0.18711755, 0.23453958, 0.22389984, 0.19660127, 0.18667151, 0.20729406, 0.23507886, 0.22921449, 0.20484409, 0.1853283, 0.23061353, 0.19133751, 0.22296222, 0.20652315, 0.19125107, 0.21160647, 0.20738243, 0.2183708, 0.22352105, 0.21633434, 0.20113727, 0.20731667, 0.20290592, 0.24475329, 0.20286429, 0.20428059, 0.21464488, 0.17589948, 0.17906964, 0.24432805, 0.19615014, 0.22176093, 0.21300325, 0.19743311, 0.23040703, 0.20381124, 0.19330302, 0.2011978, 0.21020994, 0.22404568, 0.20357245, 0.20334929, 0.21917139, 0.19162785, 0.21865368, 0.20432454, 0.23060317, 0.19750345, 0.19939415, 0.2006103, 0.22358721, 0.22190087, 0.21429239, 0.200677, 0.20934981, 0.22140308, 0.21492116, 0.24188541, 0.20013319, 0.19282003, 0.18814902, 0.21410863, 0.22847049, 0.20433973, 0.20746829, 0.19888033, 0.19968459, 0.23500797, 0.2365803, 0.20502558, 0.21796235, 0.21533354, 0.21499279, 0.19912203, 0.20456505, 0.22049418, 0.1807079, 0.234871, 0.18757775, 0.21392253, 0.22143133, 0.20542799, 0.19523798, 0.17875402, 0.19958188, 0.2168537, 0.22252147, 0.20170471, 0.2173487, 0.19524615, 0.210736, 0.20280968, 0.1897228, 0.18987973, 0.20549978, 0.20680371, 0.23416083, 0.2233334, 0.17625302, 0.20858544, 0.2048284, 0.19891314, 0.20169513, 0.23920691, 0.16980906, 0.21347506, 0.20026061, 0.23328082, 0.2442568, 0.24207914, 0.21157257, 0.22067647, 0.19161233, 0.224208, 0.21731384, 0.22071752, 0.22382267, 0.20598164, 0.2159234, 0.21990936, 0.20084898, 0.17460072, 0.2521942, 0.18362065, 0.23059589, 0.22148877, 0.19533755, 0.19149412, 0.23105249, 0.18993586, 0.2355491, 0.1992099, 0.21337119, 0.21375556, 0.19854768, 0.22858098, 0.19695209, 0.19983794, 0.21480605, 0.1907944, 0.21959627, 0.22981371, 0.19705889, 0.19570918, 0.209563, 0.20276465, 0.21108377, 0.22408135, 0.19959232, 0.21432967, 0.1889896, 0.19717489, 0.23514003, 0.1887267, 0.20479323, 0.18740404, 0.22328556, 0.22390696, 0.20205593, 0.22515532, 0.22435921, 0.2056773, 0.22952063, 0.21337172, 0.22800532, 0.2038331, 0.18094155, 0.18646695, 0.25283703, 0.20639756, 0.22897013, 0.21237691, 0.19255908, 0.18610668, 0.2262001, 0.21137519, 0.21434675, 0.19943246, 0.2248797, 0.2212846, 0.19975016, 0.22645201, 0.19989383, 0.20016561, 0.21323836, 0.20180325, 0.21615523, 0.20501001, 0.18931934, 0.23052762, 0.17247203, 0.1966417, 0.19989589, 0.23120292, 0.22518921, 0.20757848, 0.19925281, 0.21836154, 0.21275252, 0.2524254, 0.19456236, 0.23446447, 0.22339448, 0.21877225, 0.17869337, 0.22567774, 0.17632459, 0.19505046, 0.19518398, 0.23682192, 0.21526869, 0.20977159, 0.22296785, 0.20183763, 0.18448555, 0.19929296, 0.19861065, 0.19056685, 0.20376687, 0.18753469, 0.20702726, 0.23647009, 0.22430575, 0.21511966, 0.17924559, 0.2038462, 0.18789193, 0.22196098, 0.20479064, 0.20035271, 0.20716013, 0.21973112, 0.19966897, 0.18110721, 0.18208246, 0.19977514, 0.22377048, 0.20469384, 0.21769854, 0.2260972, 0.20489529, 0.22268032, 0.19160184, 0.23985086, 0.21787326, 0.2305291, 0.20626318, 0.19750945, 0.22251375, 0.21076316, 0.20833056, 0.22252364, 0.22756971, 0.218556, 0.20774023, 0.18186466, 0.18348153, 0.18954737, 0.19477277, 0.20766072, 0.20179817, 0.21430242, 0.18559392, 0.2156535, 0.22093396, 0.22548026, 0.22679469, 0.21899778, 0.19577049, 0.1867037, 0.2085168, 0.18731268, 0.22372618, 0.21341702, 0.19691497, 0.20835043, 0.19135553, 0.20277686, 0.23101175, 0.20622756, 0.19369902, 0.24125953, 0.20957856, 0.2236354, 0.21362413, 0.22687843, 0.21403977, 0.20145425, 0.20833479, 0.182183, 0.20908396, 0.21537665, 0.18803307, 0.23314993, 0.20327993, 0.19996352, 0.1935091, 0.22030106, 0.19536023, 0.20763695, 0.21494979, 0.23137286, 0.20863841, 0.22168766, 0.23143941, 0.19382204, 0.19621874, 0.2148324, 0.22245798, 0.20993182, 0.21695325, 0.20360453, 0.21216086, 0.18102546, 0.21789172, 0.2241505, 0.21167281, 0.22718671, 0.22165342, 0.2092752, 0.20396893, 0.21618684, 0.21912678, 0.23882103, 0.21577731, 0.21295756, 0.21861494, 0.21642286, 0.19939454, 0.21401574, 0.22356379, 0.19940077, 0.15876372, 0.20751935, 0.20965762, 0.23506108, 0.22803198, 0.2083097, 0.22918011, 0.1712566, 0.22945249, 0.1958407, 0.210084, 0.19239412, 0.16903694, 0.19557035, 0.1914637, 0.22350432, 0.22842953, 0.19759274, 0.21969709, 0.18023062, 0.21169354, 0.20739156, 0.18210019, 0.1964495, 0.18857321, 0.18178159, 0.21520188, 0.2150516, 0.20048587, 0.2164056, 0.2048651, 0.23572604, 0.2244225, 0.18157919, 0.1978286, 0.20019014, 0.20378321, 0.2270902, 0.22590229, 0.2164685, 0.21499498, 0.23906685, 0.18937925, 0.2161146, 0.23282406, 0.23888186, 0.21455313, 0.18191798, 0.21934094, 0.21617815, 0.19762833, 0.19919726, 0.20885336, 0.19379218, 0.24255803, 0.23713502, 0.20506273, 0.20167805, 0.2088682, 0.22121139, 0.20753852, 0.24683563, 0.19144683, 0.2012961, 0.2064021, 0.1825577, 0.24079378, 0.19809142, 0.21042083, 0.17820613, 0.22127727, 0.20993514, 0.20755218, 0.20629098, 0.22068323, 0.21525179, 0.21908265, 0.22756138, 0.19464613, 0.19567017, 0.23112519, 0.19551931, 0.22850586, 0.20420015, 0.19010675, 0.204903, 0.2243325, 0.20209907, 0.22164613, 0.20074649, 0.19418721, 0.21742435, 0.20695321, 0.2172939, 0.22200635, 0.20179904, 0.18563798, 0.22081998, 0.20485462, 0.2110328, 0.18336532, 0.20320606, 0.25634137, 0.23918408, 0.21082747, 0.18590966, 0.20430724, 0.21472333, 0.20253524, 0.21781011, 0.24121816, 0.20275407, 0.24056357, 0.21644016, 0.217976, 0.24791813, 0.20298383, 0.20121415, 0.20900036, 0.2032191, 0.19577819, 0.20633462, 0.20145294, 0.20407057, 0.18040207, 0.19967487, 0.23278709, 0.19781247, 0.2104067, 0.2502516, 0.22817962, 0.21341224, 0.21401773, 0.17671798, 0.20743491, 0.20218477, 0.21443081, 0.2057453, 0.20013246, 0.2119865, 0.18796495, 0.21150367, 0.20202473, 0.21169749, 0.21216743, 0.22356519, 0.20871362, 0.21082497, 0.21538822, 0.2081353, 0.23293088, 0.20135029, 0.21345402, 0.20183109, 0.20109494, 0.21357, 0.22458465, 0.23289898, 0.22308019, 0.1901335, 0.20138949, 0.2039809, 0.20943978, 0.20629002, 0.2275397, 0.23040181, 0.22198007, 0.21974312, 0.19239196, 0.21585667, 0.19830954, 0.1987452, 0.20238356, 0.22541606, 0.19528982, 0.23160432, 0.20441301, 0.21100013, 0.2018017, 0.22215961, 0.20043294, 0.21113622, 0.2103786, 0.21216479, 0.19597162, 0.22434498, 0.22287229, 0.23278432, 0.21344891, 0.1974154, 0.18967812, 0.18056467, 0.193661, 0.21857852, 0.21138676, 0.19240329, 0.20957166, 0.20150709, 0.21386255, 0.21301495, 0.20447299, 0.24074037, 0.19496676, 0.23258503, 0.20521341, 0.18665764, 0.18425135, 0.19164303, 0.22938292, 0.22826989, 0.19623978, 0.22117469, 0.16110139, 0.21199483, 0.21830337, 0.1984738, 0.18029124, 0.22933787, 0.2222384, 0.2319977, 0.2203717, 0.22776368, 0.22325684, 0.21508117, 0.21726573, 0.24705318, 0.17945956, 0.21055344, 0.18709305, 0.19800876, 0.25538668, 0.20073187, 0.18229651, 0.19771394, 0.2106198, 0.20549838, 0.20806967, 0.20881535, 0.20128463, 0.22405876, 0.21015131, 0.20483176, 0.22022802, 0.22402231, 0.21254985, 0.18396182, 0.18656747, 0.21808708, 0.22152406, 0.2239523, 0.20222889, 0.1963961, 0.2096887, 0.21564619, 0.19864634, 0.21600202, 0.20647176, 0.20447, 0.20347282, 0.21650493, 0.22766313, 0.16879122, 0.21848172, 0.18632832, 0.1980302, 0.24356839, 0.18926637, 0.19808842, 0.21981539, 0.21488467, 0.19072671, 0.21606772, 0.22139277, 0.20621148, 0.19173878, 0.18815055, 0.17410447, 0.20570421, 0.17951831, 0.22693048, 0.22327673, 0.23272349, 0.2010053, 0.23120221, 0.20394538, 0.2222429, 0.23455243, 0.19980887, 0.22480987, 0.19565761, 0.19900122, 0.22656836, 0.1841891, 0.18584622, 0.21499398, 0.21166942, 0.20001762, 0.19824101, 0.21676119, 0.21008447, 0.21103841, 0.21511346, 0.19207323, 0.18898237, 0.22063625, 0.23025544, 0.22787182, 0.18632635, 0.230036, 0.21309754, 0.21417068, 0.2267411, 0.20963682, 0.20886758, 0.21793106, 0.21856013, 0.20725952, 0.18566234, 0.20252271, 0.20412402, 0.22033383, 0.20236106, 0.22234222, 0.24614565, 0.21316187, 0.23304966, 0.17326114, 0.184109, 0.19414893, 0.20248327, 0.2033968, 0.24559473, 0.20438667, 0.20425269, 0.17441718, 0.20041248, 0.2168179, 0.21769056, 0.21437186, 0.1968832, 0.2385689, 0.21724676, 0.22750689, 0.20509239, 0.23623212, 0.21579273, 0.2077029, 0.21737806, 0.20587747, 0.20767134, 0.2208721, 0.17850818, 0.20701413, 0.20695803, 0.19031207, 0.19173463, 0.22966532, 0.2270657, 0.2237988, 0.22301799, 0.21473612, 0.2001776, 0.1940726, 0.19685407, 0.22596823, 0.19531684, 0.21182212, 0.20697553, 0.20031184, 0.23513876, 0.19104269, 0.23483914, 0.21624206, 0.23380575, 0.2096633, 0.2174739, 0.19858792, 0.19152828, 0.21957195, 0.21323764, 0.20634425, 0.22151634, 0.21583198, 0.21475384, 0.1764299, 0.21343282, 0.26890442, 0.2161664, 0.20473546, 0.22090653, 0.22868715, 0.18154004, 0.20340252, 0.19256774, 0.2237529, 0.2133245, 0.2033672, 0.19137137, 0.20821412, 0.21566139, 0.20258367, 0.23062728, 0.19294734, 0.21034314, 0.19475123, 0.20525515, 0.19955108, 0.20732489, 0.19434772, 0.19792634, 0.19782452, 0.19178368, 0.21421996, 0.18716468, 0.21668893, 0.21878389, 0.23096755, 0.20499425, 0.22907126, 0.21710631, 0.22167042, 0.21291502, 0.17777486, 0.17674035, 0.22234254, 0.2028409, 0.21427777, 0.22615778, 0.17148778, 0.20065732, 0.21260244, 0.18960173, 0.22615342, 0.20213763, 0.1792351, 0.21255125, 0.21090253, 0.23172924, 0.20519587, 0.18556497, 0.21176863, 0.2262289, 0.23609243, 0.21889046, 0.2142869, 0.19807966, 0.22239077, 0.2087946, 0.19675367, 0.19755399, 0.20455183, 0.22608453, 0.2385412, 0.21847357, 0.21903828, 0.20970795, 0.22560726, 0.21360631, 0.20780732, 0.2116993, 0.21741724, 0.21468808, 0.19414793, 0.20721018, 0.18068343, 0.22247049, 0.249856, 0.2004873, 0.20736672, 0.20730598, 0.20875709, 0.21258064, 0.2037275, 0.22636007, 0.19257936, 0.19542268, 0.22826108, 0.23013826, 0.23521455, 0.23516233, 0.21626122, 0.20449898, 0.20346165, 0.22204463, 0.20510392, 0.21306562, 0.257353, 0.19762461, 0.2222782, 0.19921772, 0.20019276, 0.17924735, 0.19916525, 0.21331388, 0.21376497, 0.2231437, 0.19420029, 0.20427518, 0.18395574, 0.19791208, 0.18932216, 0.196604, 0.187005, 0.20260942, 0.23199926, 0.20761745, 0.20136964, 0.19610946, 0.23348185, 0.20253575, 0.20263386, 0.20707993, 0.2281327, 0.23245643, 0.20573685, 0.23518732, 0.22057438, 0.20685568, 0.16937958, 0.21342805, 0.1741577, 0.21395211, 0.18676004, 0.22079308, 0.1884958, 0.19155653, 0.21616223, 0.22178867, 0.21386601, 0.19222559, 0.1925898, 0.2495252, 0.22892539, 0.24919201, 0.21655485, 0.21610346, 0.24390945, 0.1834728, 0.21725473, 0.20006254, 0.22558996, 0.18977152, 0.21179244, 0.20540407, 0.20717447, 0.23808298, 0.23296243, 0.18536083, 0.1859856, 0.19839971, 0.22555664, 0.20707686, 0.17952521, 0.1958436, 0.19383676, 0.20223077, 0.23518276, 0.21268323, 0.21103092, 0.23204124, 0.19099939, 0.19986248, 0.20170857, 0.23109376, 0.21247746, 0.20105514, 0.20067552, 0.1836232, 0.23672836, 0.2150768, 0.19485347, 0.18382981, 0.20846736, 0.23039277, 0.20804115, 0.21170205, 0.229278, 0.19394666, 0.19183093, 0.20786476, 0.20165911, 0.19452594, 0.21517265, 0.22009364, 0.19135825, 0.22570089, 0.21698359, 0.18614283, 0.19223917, 0.20082556, 0.20371243, 0.24200422, 0.21794756, 0.205235, 0.21423358, 0.18832983, 0.20415722, 0.20962538, 0.19668597, 0.2042757, 0.20337035, 0.20634817, 0.18845293, 0.18806176, 0.22054182, 0.21580406, 0.23266861, 0.2142992, 0.19968574, 0.20366147, 0.2295686, 0.17013346, 0.22879155, 0.20131533, 0.19019763, 0.20378572, 0.20461509, 0.21345052, 0.2256318, 0.21088263, 0.22061679, 0.204361, 0.19847748, 0.19469075, 0.20327532, 0.19317016, 0.19706145, 0.22260502, 0.18297186, 0.22726656, 0.20968468, 0.20320651, 0.20864049, 0.20472649, 0.20151475, 0.23627758, 0.20054272, 0.2429116, 0.23751718, 0.22642235, 0.18154444, 0.20664567, 0.20974918, 0.21276258, 0.20073132, 0.19931667, 0.23590824, 0.21265142, 0.20727466, 0.2010639, 0.20186898, 0.2180529, 0.19811036, 0.21085422, 0.22435483, 0.21722308, 0.219648, 0.19108738, 0.2226187, 0.20191738, 0.20240098, 0.24604598, 0.2379789, 0.20534065, 0.18980636, 0.22127111, 0.23247144, 0.21531868, 0.21482703, 0.19971068, 0.22169907, 0.23466179, 0.19511902, 0.20189528, 0.19921264, 0.22032616, 0.19536631, 0.2209583, 0.18964455, 0.2100198, 0.22064538, 0.20294544, 0.19756272, 0.17812106, 0.21536995, 0.24770316, 0.21791722, 0.19919392, 0.21308513, 0.24166137, 0.23175947, 0.21158275, 0.2193698, 0.20109722, 0.19623575, 0.21457584, 0.21777223, 0.1886799, 0.20229703, 0.23223062, 0.19374353, 0.21380906, 0.20033047, 0.18271199, 0.21796903, 0.19943576, 0.2324992, 0.21067736, 0.20857221, 0.18378887, 0.22436358, 0.20065993, 0.23519377, 0.18841341, 0.20545492, 0.19288349, 0.21016707, 0.22716744, 0.22018658, 0.22010244, 0.23161007, 0.20223427, 0.21445495, 0.21206683, 0.19904532, 0.20866221, 0.21993628, 0.20028865, 0.20664097, 0.2038357, 0.21298522, 0.22799206, 0.21413063, 0.16265541, 0.2137399, 0.2053354, 0.17728536, 0.21729778, 0.19041784, 0.24155408, 0.19463322, 0.21992138, 0.21718286, 0.1986631, 0.19347164, 0.19334511, 0.22159073, 0.19288008, 0.21806183, 0.21487735, 0.23619512, 0.20460111, 0.21868646, 0.21706632, 0.21911772, 0.22644542, 0.20094663, 0.22291413, 0.20882425, 0.18537061, 0.22618277, 0.18466634, 0.201556, 0.2275349, 0.1983826, 0.21247232, 0.24960123, 0.2239099, 0.19724764, 0.21716663, 0.23555946, 0.19074433, 0.20797011, 0.2005963, 0.24035858, 0.2260184, 0.18225302, 0.23631755, 0.21382982, 0.20786303, 0.21296284, 0.19413042, 0.20542365, 0.2238225, 0.21290386, 0.20746644, 0.20550385, 0.20108098, 0.21583535, 0.22865069, 0.22082542, 0.21669042, 0.21402605, 0.19737995, 0.18317182, 0.18853988, 0.22683738, 0.22489236, 0.20948188, 0.21952398, 0.18849444, 0.24821317, 0.18128844, 0.18232617, 0.21403322, 0.19186027, 0.21776272, 0.2000345, 0.19413082, 0.24442261, 0.20103328, 0.22652544, 0.1856724, 0.1847108, 0.23465039, 0.21752162, 0.18140994, 0.18003549, 0.22120267, 0.21435854, 0.20360093, 0.18077171, 0.23690541, 0.19561604, 0.21515751, 0.22046389, 0.21688947, 0.21357596, 0.21660575, 0.21222839, 0.2009629, 0.19798052, 0.21609566, 0.18543935, 0.22011398, 0.21972373, 0.17548396, 0.20984446, 0.22401437, 0.2310962, 0.21320769, 0.20430009, 0.20036395, 0.2145162, 0.1914232, 0.20801839, 0.21834514, 0.23265222, 0.19588992, 0.20546795, 0.22443268, 0.19781335, 0.18648799, 0.20908815, 0.19560832, 0.23992842, 0.22444418, 0.19712283, 0.214746, 0.19966729, 0.20914984, 0.19068536, 0.19616534, 0.22827503, 0.19399798, 0.21335377, 0.21727496, 0.20124124, 0.18521182, 0.20483051, 0.20158482, 0.20443185, 0.2265355, 0.17036188, 0.22467466, 0.19864021, 0.18830767, 0.2194513, 0.19637412, 0.21212348, 0.2390481, 0.18073973, 0.22345534, 0.22548419, 0.20103827, 0.21674764, 0.23187251, 0.2096678, 0.22423458, 0.21198785, 0.17127219, 0.19298583, 0.2054931, 0.19603507, 0.22340804, 0.20831971, 0.20899795, 0.23105921, 0.20649295, 0.2082985, 0.19825861, 0.23126394, 0.23666589, 0.2191312, 0.19044411, 0.22404693, 0.19144826]\n",
            "[0.21174388, 0.19864559, 0.19319585, 0.18715192, 0.19354768, 0.19497377, 0.2184144, 0.20882632, 0.1972462, 0.20122442, 0.2121442, 0.19742307, 0.18998127, 0.21476541, 0.19687037, 0.22293116, 0.21952075, 0.21205854, 0.19783843, 0.20856632, 0.20319302, 0.19710442, 0.23720269, 0.21723534, 0.19625972, 0.22292463, 0.20618747, 0.18278667, 0.17958264, 0.2203761, 0.19548857, 0.18405649, 0.20536336, 0.18320414, 0.20598625, 0.17464001, 0.22725078, 0.21181004, 0.19971965, 0.19223087, 0.1917441, 0.17850254, 0.19225445, 0.20160249, 0.20944187, 0.18440941, 0.19283178, 0.21619399, 0.1901646, 0.19962479, 0.21406357, 0.19941957, 0.22193033, 0.2087861, 0.19774564, 0.22464834, 0.19658393, 0.18257034, 0.22020817, 0.21917824, 0.19863558, 0.19891699, 0.2019329, 0.20533735, 0.19195257, 0.18723266, 0.22226703, 0.21072376, 0.20504102, 0.23070785, 0.20348641, 0.21545887, 0.19988601, 0.21264648, 0.17500614, 0.18034026, 0.20365474, 0.20711985, 0.18692358, 0.18078405, 0.21784557, 0.2197338, 0.22011235, 0.18605813, 0.18404703, 0.21582368, 0.19467497, 0.2057906, 0.19522902, 0.19093522, 0.20269902, 0.201934, 0.1952454, 0.18905859, 0.21414289, 0.18524654, 0.19733624, 0.20594202, 0.21571575, 0.19802566, 0.18393292, 0.22290134, 0.19671822, 0.18416558, 0.19023602, 0.21082184, 0.2065148, 0.2329252, 0.231128, 0.2110151, 0.18120249, 0.21113573, 0.21809582, 0.18346255, 0.17375422, 0.1899664, 0.22558358, 0.21866412, 0.20546748, 0.17287458, 0.21942899, 0.18733406, 0.21943717, 0.19414221, 0.18431936, 0.20678502, 0.20518115, 0.19623797, 0.19719407, 0.21577466, 0.19310853, 0.19039159, 0.18219419, 0.22090137, 0.18482889, 0.1976431, 0.2004375, 0.17105946, 0.1805108, 0.21822932, 0.19479015, 0.20220922, 0.20804985, 0.1901289, 0.21829614, 0.1948971, 0.19402708, 0.1862436, 0.19033745, 0.22224903, 0.187974, 0.1815453, 0.21159115, 0.19219455, 0.20987646, 0.20403676, 0.2239577, 0.20207682, 0.20241216, 0.19741209, 0.22503126, 0.20321815, 0.21062687, 0.18908583, 0.20569824, 0.21223886, 0.20570424, 0.21026686, 0.19255586, 0.1893154, 0.19171345, 0.21109197, 0.22865447, 0.20425141, 0.19229344, 0.19139607, 0.18630722, 0.20626622, 0.23012336, 0.19549903, 0.20971234, 0.21160743, 0.22645877, 0.19056454, 0.19979845, 0.21048428, 0.1770924, 0.21813747, 0.19258104, 0.21306598, 0.19691052, 0.19107628, 0.18366288, 0.18514471, 0.1942394, 0.20640834, 0.22369844, 0.2095366, 0.22160375, 0.18488134, 0.20051171, 0.19672562, 0.18243566, 0.17764352, 0.20471475, 0.20805514, 0.22253025, 0.21311547, 0.17202832, 0.20778187, 0.19960873, 0.19042253, 0.20123807, 0.22171606, 0.17039673, 0.21035282, 0.20290133, 0.20783404, 0.22107951, 0.22192106, 0.19713077, 0.20948505, 0.19084594, 0.20454152, 0.19987461, 0.20511037, 0.21507572, 0.19201943, 0.20282733, 0.2058229, 0.19320023, 0.16472171, 0.22589485, 0.17818053, 0.21451066, 0.21070662, 0.18781051, 0.17627604, 0.21050717, 0.17844789, 0.20952833, 0.17159332, 0.2042568, 0.19182664, 0.20666496, 0.20715511, 0.2049779, 0.20036085, 0.21541804, 0.18514863, 0.22057307, 0.20783654, 0.19284128, 0.19226459, 0.21325618, 0.20117824, 0.19625469, 0.20820233, 0.18738246, 0.20971277, 0.194362, 0.19162291, 0.21143514, 0.1841064, 0.20057704, 0.17414176, 0.21300866, 0.18491279, 0.2060487, 0.20346066, 0.20944591, 0.18989539, 0.20825218, 0.1980116, 0.19780438, 0.18474352, 0.17821887, 0.16741078, 0.23479752, 0.18804368, 0.20838118, 0.20745368, 0.17874233, 0.18323855, 0.21226303, 0.20969526, 0.20832679, 0.19671483, 0.20767398, 0.21434262, 0.18248467, 0.20602128, 0.18402867, 0.20157015, 0.20457354, 0.18816659, 0.22496726, 0.19486319, 0.18698792, 0.22207247, 0.17271255, 0.18211108, 0.20684645, 0.22071421, 0.2155832, 0.19686647, 0.18971092, 0.21481904, 0.21119095, 0.23452069, 0.18780324, 0.21110031, 0.21584661, 0.21081668, 0.17918527, 0.20692709, 0.1812015, 0.18834655, 0.18183382, 0.21020599, 0.20462677, 0.22046325, 0.2041007, 0.1978771, 0.17711385, 0.19589186, 0.18265526, 0.18259273, 0.20037134, 0.19115834, 0.19928356, 0.23108041, 0.19823296, 0.19725305, 0.18290277, 0.1986081, 0.17826875, 0.20318504, 0.19508083, 0.20060934, 0.20194528, 0.20529807, 0.19055466, 0.17728294, 0.18601431, 0.20233771, 0.2032634, 0.19302459, 0.21433985, 0.21412082, 0.19905072, 0.2067559, 0.19974127, 0.21065864, 0.19409648, 0.21086016, 0.19276369, 0.19831236, 0.212285, 0.20833574, 0.19179262, 0.19577754, 0.21779901, 0.21500246, 0.20319498, 0.17652875, 0.17496279, 0.2001152, 0.19136675, 0.20020965, 0.18791506, 0.19667669, 0.17889006, 0.20659812, 0.21139722, 0.21173364, 0.22376789, 0.20560274, 0.17926267, 0.1869583, 0.17113101, 0.1842675, 0.21089432, 0.19768801, 0.20030984, 0.20450437, 0.18409064, 0.19440442, 0.21033232, 0.18714055, 0.18476605, 0.2251766, 0.18984997, 0.20548643, 0.20384738, 0.21446273, 0.19236085, 0.20423758, 0.19828156, 0.17819887, 0.18839371, 0.20352332, 0.18820253, 0.21647292, 0.19299468, 0.20008577, 0.19547474, 0.1927127, 0.19455399, 0.20945624, 0.20402752, 0.2095381, 0.20942315, 0.21150248, 0.21797112, 0.1862336, 0.19304827, 0.21310239, 0.20680784, 0.20855622, 0.19972351, 0.19303806, 0.19772278, 0.18331763, 0.20572454, 0.21861856, 0.19004348, 0.21261457, 0.20883462, 0.19338985, 0.19754133, 0.19871204, 0.18699996, 0.22081652, 0.19986121, 0.20141304, 0.20741194, 0.20436604, 0.18445663, 0.20544663, 0.20189698, 0.19123352, 0.16446477, 0.21897125, 0.1965975, 0.21735539, 0.22046511, 0.19176252, 0.20912245, 0.18156278, 0.20799835, 0.19065294, 0.20440987, 0.17099714, 0.17640416, 0.20092891, 0.18267384, 0.20254725, 0.20120491, 0.19555187, 0.21365683, 0.1792286, 0.20160873, 0.19444492, 0.17835999, 0.20285833, 0.18842737, 0.17646122, 0.20205788, 0.20421275, 0.19068609, 0.20704642, 0.2036033, 0.23225443, 0.21519834, 0.17923577, 0.1903334, 0.18784904, 0.18725745, 0.21747343, 0.20929427, 0.20953669, 0.21875694, 0.21540391, 0.19238898, 0.20881453, 0.2189629, 0.23038028, 0.19764775, 0.19140851, 0.20232196, 0.1986738, 0.18780439, 0.18313272, 0.18558294, 0.19766061, 0.23311143, 0.21716487, 0.21035783, 0.19871916, 0.19360615, 0.21651454, 0.2016233, 0.2362224, 0.17496069, 0.19164085, 0.1993683, 0.17420964, 0.20788842, 0.18888344, 0.22079122, 0.18738973, 0.20388453, 0.1979892, 0.20162089, 0.20522785, 0.19629157, 0.20666568, 0.21661372, 0.21298622, 0.19593343, 0.19541836, 0.2070925, 0.1861128, 0.2138126, 0.19427676, 0.17883171, 0.19671726, 0.21517013, 0.19686654, 0.20618898, 0.20222804, 0.1905147, 0.2142071, 0.19770929, 0.20046689, 0.20518371, 0.2001644, 0.18397057, 0.21123746, 0.20583765, 0.1971257, 0.18221654, 0.19744405, 0.23222068, 0.21545765, 0.16949202, 0.1823163, 0.18972138, 0.20264925, 0.19776249, 0.20366564, 0.22583917, 0.20114149, 0.23102903, 0.1983343, 0.2073929, 0.22895168, 0.19791368, 0.19656189, 0.20200264, 0.20424768, 0.18655512, 0.19267142, 0.19993605, 0.19954763, 0.1790599, 0.18025048, 0.22426224, 0.1918909, 0.2004792, 0.23904617, 0.20159487, 0.21685496, 0.19750786, 0.17946522, 0.20217358, 0.1995844, 0.19585219, 0.19424075, 0.20926398, 0.21179044, 0.19004549, 0.19821784, 0.19360565, 0.2041049, 0.19863756, 0.21354125, 0.20078042, 0.20152947, 0.18287821, 0.20285356, 0.22161147, 0.21214649, 0.21077278, 0.19270225, 0.19276953, 0.21373418, 0.20548694, 0.22097075, 0.2092713, 0.1791407, 0.20095408, 0.20682453, 0.20318244, 0.19129194, 0.21702285, 0.20629081, 0.21750662, 0.2040898, 0.20598538, 0.20087796, 0.19374718, 0.18671009, 0.20208623, 0.22056375, 0.1919748, 0.230631, 0.19416913, 0.20275606, 0.19883844, 0.20391382, 0.19223668, 0.20594189, 0.20793903, 0.20071767, 0.18973452, 0.2222091, 0.2108234, 0.23197582, 0.19513798, 0.20260274, 0.18806477, 0.18592145, 0.19534394, 0.20146513, 0.18729943, 0.19921468, 0.19697087, 0.18889341, 0.20516822, 0.18768375, 0.20260856, 0.2152511, 0.20208055, 0.22155368, 0.20262371, 0.17986362, 0.18644544, 0.18395278, 0.22041671, 0.21125162, 0.17525212, 0.20522821, 0.16172245, 0.19125535, 0.19065276, 0.2104532, 0.17917986, 0.21805447, 0.20293912, 0.21199222, 0.19715163, 0.215572, 0.19956347, 0.22057965, 0.2072529, 0.21870272, 0.18106663, 0.18882361, 0.17833109, 0.20074347, 0.23188227, 0.19945985, 0.18077455, 0.20239213, 0.19921775, 0.2132378, 0.20075047, 0.18630369, 0.18042521, 0.21862482, 0.20648593, 0.19702709, 0.22026455, 0.21143886, 0.20671679, 0.17333603, 0.19143093, 0.20777825, 0.20932814, 0.21949612, 0.19725902, 0.18976668, 0.20803474, 0.20534334, 0.17111957, 0.20680329, 0.19941582, 0.19491962, 0.19161841, 0.21184127, 0.21400274, 0.17697689, 0.2109919, 0.18694435, 0.18981856, 0.22637059, 0.177767, 0.19098899, 0.20605294, 0.20511329, 0.18383135, 0.19505909, 0.21142587, 0.20030437, 0.18314302, 0.17744501, 0.18772338, 0.20275325, 0.17499617, 0.21378954, 0.20516121, 0.21725295, 0.19836962, 0.22079858, 0.19140598, 0.21165492, 0.21619186, 0.18451166, 0.20726702, 0.20341891, 0.19698738, 0.20532581, 0.18101187, 0.18718974, 0.21787766, 0.18647197, 0.18235072, 0.19479123, 0.20924672, 0.18869752, 0.1912149, 0.21406028, 0.18525444, 0.18234351, 0.21260083, 0.21176066, 0.21142769, 0.17708322, 0.2098823, 0.19126987, 0.20161027, 0.20764945, 0.1898328, 0.19767144, 0.20798042, 0.19954287, 0.22218677, 0.17235163, 0.19643566, 0.18272054, 0.20185857, 0.19128388, 0.21984513, 0.20628677, 0.21116379, 0.22900191, 0.1789628, 0.18923943, 0.19394596, 0.1957902, 0.19245274, 0.22471172, 0.1905965, 0.18767715, 0.17794193, 0.19012478, 0.20964982, 0.20499684, 0.19397949, 0.19911906, 0.2143819, 0.20346498, 0.20670263, 0.20790507, 0.22968613, 0.20201342, 0.20130366, 0.19613193, 0.19206476, 0.20610745, 0.21231174, 0.1733036, 0.19679315, 0.20327318, 0.1851527, 0.19050364, 0.23216607, 0.20463851, 0.20717739, 0.21470433, 0.21059945, 0.18873106, 0.20207573, 0.19101746, 0.19822997, 0.2072959, 0.18805447, 0.20156154, 0.19582307, 0.21085136, 0.18805206, 0.22722326, 0.21153943, 0.20152271, 0.19475377, 0.19991507, 0.18270011, 0.19333081, 0.21840078, 0.21070507, 0.19641677, 0.20791808, 0.20492913, 0.19881366, 0.17537855, 0.20447929, 0.24874379, 0.20927484, 0.19026895, 0.20152716, 0.21919358, 0.16936864, 0.2026058, 0.18215814, 0.2199261, 0.20591557, 0.19677414, 0.19745438, 0.19673774, 0.20661046, 0.19816308, 0.22151352, 0.18508989, 0.18634291, 0.19403842, 0.17574684, 0.20074593, 0.20508039, 0.18023825, 0.19353803, 0.20475751, 0.1884054, 0.20339893, 0.16927436, 0.20747775, 0.1957886, 0.22199999, 0.1998297, 0.2107064, 0.20993784, 0.20364761, 0.19777606, 0.17082272, 0.18271483, 0.21613441, 0.18936741, 0.214369, 0.21727741, 0.17962754, 0.19553144, 0.20740855, 0.17779875, 0.19623768, 0.19778672, 0.1684122, 0.21402359, 0.1984614, 0.20026189, 0.20160143, 0.17796084, 0.19665273, 0.19843538, 0.21334907, 0.2080812, 0.20544465, 0.18963107, 0.20303947, 0.22182934, 0.19222537, 0.19329035, 0.19993336, 0.20021214, 0.2235146, 0.21216634, 0.20329225, 0.20378031, 0.21262328, 0.20575874, 0.20177023, 0.1988708, 0.2039767, 0.2197567, 0.19095816, 0.19180484, 0.17781398, 0.21418735, 0.23355666, 0.20186761, 0.19298795, 0.19595964, 0.18684247, 0.20201774, 0.19205652, 0.2064813, 0.19921346, 0.19392715, 0.21627031, 0.21850938, 0.22076975, 0.2155405, 0.19770223, 0.19197547, 0.19319126, 0.20179503, 0.19724576, 0.2116501, 0.22038443, 0.19754894, 0.21612145, 0.20464066, 0.19265044, 0.16546857, 0.19816026, 0.1936167, 0.20253481, 0.20691292, 0.17977735, 0.20278358, 0.18914798, 0.1814957, 0.18182719, 0.20083977, 0.18359482, 0.19039372, 0.220696, 0.17972347, 0.1894931, 0.18822874, 0.223378, 0.20056993, 0.18809454, 0.20058592, 0.20191266, 0.2106716, 0.1846366, 0.22436051, 0.19658339, 0.20560642, 0.17221199, 0.21330678, 0.17974098, 0.21112697, 0.18232039, 0.21101288, 0.18932806, 0.19958033, 0.19146919, 0.2099539, 0.231336, 0.19625725, 0.2016296, 0.22525106, 0.21544686, 0.23140495, 0.20607762, 0.21673024, 0.21380211, 0.18519232, 0.19733238, 0.1945698, 0.2062577, 0.17989865, 0.21240231, 0.21280682, 0.19478625, 0.20775695, 0.22136194, 0.18833081, 0.1727002, 0.17921427, 0.21107127, 0.19920619, 0.17270324, 0.18332365, 0.18937379, 0.19770586, 0.2211821, 0.19519515, 0.19875917, 0.21058269, 0.1891476, 0.19743863, 0.1885116, 0.19814128, 0.20585941, 0.19155452, 0.1954868, 0.17787446, 0.21884124, 0.19808306, 0.19062956, 0.1937786, 0.19723363, 0.21066006, 0.19938333, 0.18381372, 0.20862854, 0.17868467, 0.19363335, 0.19590528, 0.18828972, 0.19662735, 0.2057532, 0.19745988, 0.18762466, 0.20746377, 0.20609848, 0.1823996, 0.1803517, 0.19355556, 0.20353773, 0.22395056, 0.206657, 0.20454307, 0.21419205, 0.1770705, 0.20464343, 0.20556416, 0.20433596, 0.19511716, 0.19959484, 0.1912082, 0.17483933, 0.1853962, 0.22005685, 0.19615892, 0.23337156, 0.20466664, 0.18305638, 0.20776857, 0.20992763, 0.16897643, 0.21014482, 0.19750659, 0.17818895, 0.1915617, 0.19470215, 0.18555987, 0.21006349, 0.19492862, 0.20887125, 0.20851763, 0.192364, 0.19613118, 0.20242774, 0.18371049, 0.19062524, 0.2104699, 0.18254174, 0.2136898, 0.20341657, 0.18651168, 0.19085723, 0.21262707, 0.19589406, 0.21097858, 0.19986022, 0.23461342, 0.22898147, 0.22368462, 0.18656228, 0.2077114, 0.19796778, 0.20711383, 0.2122586, 0.19094364, 0.20827429, 0.20585689, 0.21498798, 0.20015539, 0.18766823, 0.20586473, 0.18940872, 0.19852793, 0.2148702, 0.20622846, 0.21650001, 0.18409747, 0.2127884, 0.2004593, 0.18710531, 0.23639466, 0.20757067, 0.20274071, 0.19093059, 0.2006088, 0.19827747, 0.20333122, 0.20481393, 0.18610504, 0.20486328, 0.20421909, 0.18717934, 0.19331673, 0.18084376, 0.21065205, 0.19341275, 0.20624946, 0.18549435, 0.20954618, 0.20102088, 0.19271637, 0.19561377, 0.1827009, 0.21837433, 0.22544806, 0.19836842, 0.19171654, 0.19649583, 0.22563535, 0.22195448, 0.20357046, 0.19774008, 0.19585773, 0.19854936, 0.1883131, 0.20715944, 0.19670898, 0.19098563, 0.2177447, 0.19857489, 0.19778052, 0.19375661, 0.18945517, 0.22271195, 0.19517776, 0.20332146, 0.20129035, 0.19491707, 0.17606823, 0.20228381, 0.18499869, 0.22055821, 0.19245802, 0.2062429, 0.18915609, 0.20203993, 0.21235774, 0.20078632, 0.21307275, 0.21627457, 0.20186538, 0.20434377, 0.20373142, 0.1918156, 0.20695364, 0.2055432, 0.19624832, 0.19163711, 0.19192484, 0.19959776, 0.2220704, 0.20164235, 0.1694701, 0.2002038, 0.19374087, 0.18818572, 0.20710915, 0.17663081, 0.2178516, 0.16893114, 0.20969655, 0.21676363, 0.19403656, 0.18967794, 0.20109013, 0.20094004, 0.19405143, 0.21675552, 0.2092836, 0.22269747, 0.20227292, 0.20621336, 0.21683547, 0.21426941, 0.20570903, 0.19551685, 0.21022265, 0.20153758, 0.17916355, 0.20954625, 0.17235331, 0.19984153, 0.21606477, 0.18874879, 0.19201176, 0.22753282, 0.20378758, 0.19193259, 0.2131965, 0.21050242, 0.20135307, 0.20922974, 0.20215178, 0.2166133, 0.21542804, 0.17051387, 0.2223538, 0.21793447, 0.19578964, 0.19982202, 0.18861973, 0.19158567, 0.19591305, 0.19941239, 0.20769614, 0.2147705, 0.1845215, 0.21136127, 0.21053618, 0.19866797, 0.20319891, 0.20659365, 0.18718478, 0.18180558, 0.18955842, 0.20430028, 0.21721503, 0.19276065, 0.20944808, 0.1886839, 0.21374144, 0.19454332, 0.18209349, 0.20742051, 0.18073055, 0.19524734, 0.18325464, 0.17868182, 0.23325889, 0.18781288, 0.22007799, 0.17789283, 0.18018243, 0.20585138, 0.21292253, 0.17954947, 0.17888278, 0.20565337, 0.2013558, 0.18864708, 0.1789562, 0.23082083, 0.19099829, 0.21440141, 0.20576881, 0.20424162, 0.20777757, 0.20219998, 0.206665, 0.20120437, 0.1987931, 0.21591768, 0.18021853, 0.19830683, 0.21645142, 0.17733447, 0.20358163, 0.22726195, 0.20680423, 0.21459658, 0.19592263, 0.20408805, 0.21348229, 0.18467388, 0.19430688, 0.21346729, 0.21017005, 0.18962038, 0.20842443, 0.20769612, 0.17405698, 0.18740632, 0.2036229, 0.19084099, 0.21561635, 0.20144549, 0.1981333, 0.20436646, 0.19862516, 0.20753548, 0.1904475, 0.19519588, 0.20927498, 0.17781058, 0.20162188, 0.21146592, 0.17847694, 0.17921956, 0.19329627, 0.2007447, 0.20260331, 0.21070103, 0.16761246, 0.20104234, 0.19866574, 0.18484658, 0.20815663, 0.19431119, 0.19903778, 0.21687691, 0.18571596, 0.20772782, 0.2104276, 0.1846368, 0.2056971, 0.23173162, 0.19503966, 0.19878146, 0.20137519, 0.17255998, 0.19092104, 0.2075384, 0.19893646, 0.19617578, 0.20813243, 0.20444629, 0.20138611, 0.20425117, 0.19904003, 0.20464036, 0.2215363, 0.22962822, 0.21147814, 0.18103865, 0.22733311, 0.19764593]\n",
            "0.06049194194934296\n"
          ]
        }
      ]
    }
  ]
}